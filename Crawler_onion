import requests
from bs4 import BeautifulSoup

def extract(content):
    soup = BeautifulSoup(content)
    tag = soup.find('title', text=True)

    if not tag:
        return None

    return tag.string.strip()

def extract_link(content):
    soup = BeautifulSoup(content)
    link = set()
    
    for tag in soup.find_all('a', href=True):
        if '.onion' in tag['href']:
            link.add(tag['href'])
    return link

def craw(start_url):
    global search
    url_vista = set([start_url])
    url_nao_vista = set([start_url])

    print('#Verificando links:\n')
    
    session = requests.session()
    session.proxies = {'http':  'socks5h://127.0.0.1:9150',
                   'https': 'socks5h://127.0.0.1:9150'}
    
    while url_nao_vista:
        url = url_nao_vista.pop()
        try:
            content = session.get(url, timeout=50)
            if content.status_code == 200:
                title = extract(content.text)
                print(title)
                print(url)
                print()                        
                with open('links.txt', 'a') as doc:
                    doc.write('Title: %s\nLink: %s\n'%(title, url))
                    
        except Exception as error:
            print('Tempo limite de conex√£o expirada! Tentando novamente.')
            url_vista.add(url)

        try:
            
            for link in extract_link(content.text):
                if link not in url_vista:
                    url_nao_vista.add(link)
                else:
                    url_vista.add(link)
        except Exception as error:
            print('Verifique se o Tor Browser esta aberto e tente novamente!')
            print('#################################################\n\n')
            main()
            
def main():
    try:
        alvo = str(input('CRAWLER: '))
        if alvo.startswith('http://'):
            craw(alvo)
        else:
            craw('http://'+alvo)
    except KeyboardInterrupt:
        print()
        print('Bye')
main()
